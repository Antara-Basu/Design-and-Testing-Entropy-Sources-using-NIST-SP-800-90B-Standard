{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antara-Basu/Design-and-Testing-Entropy-Sources-using-NIST-SP-800-90B-Standard/blob/main/Entropy_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entropy Estimation for  IID and Non-IID Data\n"
      ],
      "metadata": {
        "id": "OBn60I4aDd2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For IID data and non-IID data, the following estimators shall be calculated on the outputs of the noise source and the minimum of all the estimates is taken as the entropy assessment of the entropy source for this Recommendation"
      ],
      "metadata": {
        "id": "mDQSt2SwDfCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)The Most Common Value Estimate\n"
      ],
      "metadata": {
        "id": "nGduldRHAPYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def most_freq(s):\n",
        "\n",
        "  \"\"\"\n",
        "  This function calculates the frequency of the most common element occuring in input s.\n",
        "  @input: s: input data\n",
        "  @output: max_count: frequency of the most common element of the input s.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  count = {}\n",
        "  for i in s:\n",
        "    if count.get(i):\n",
        "      count[i] += 1\n",
        "    else:\n",
        "      count[i] = 1\n",
        "  max_count = max(count.values())\n",
        "  return max_count\n",
        "\n",
        "def most_common_value_estimate(s):\n",
        "\n",
        "  \"\"\"\n",
        "  This method first finds the proportion 𝑝̂ of the most common value in the input dataset, and then constructs a confidence interval for this proportion. The upper bound of the confidence interval is used to estimate the min-entropy per sample of the source.\n",
        "  @input: s: input data\n",
        "  @output: min_entropy: min_entropy of the input s.\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(s)\n",
        "  p = most_freq(s)/L #Step 1: Find the proportion of the most common value p in the dataset.\n",
        "  p_dash = min(1, p + 2.576*math.sqrt((p*(1-p))/(L-1))) #Step 2: Calculate an upper bound on the probability of the most common value p_dash as\n",
        "  min_entropy = -math.log(p_dash, 2) #Step 3: The estimated min-entropy\n",
        "  return p, p_dash, min_entropy\n",
        "\n",
        "s = (0, 1, 1, 2, 0, 1, 2, 2, 0, 1, 0, 1, 1, 0, 2, 2, 1, 0, 2, 1)\n",
        "common_value = most_common_value_estimate(s)\n",
        "print(common_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymLtaYXlAYbN",
        "outputId": "0b04f055-50a9-4e63-8fb0-c34a1ac9cd0c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.4, 0.6895174060761333, 0.5363411235066873)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)The Markov Estimate"
      ],
      "metadata": {
        "id": "nWyJxqrMAmBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def count(s, character):# This function counts the occurence of the given character in s.\n",
        "    occ = 0\n",
        "    for c in s:\n",
        "        if c == character:\n",
        "            occ += 1\n",
        "    return occ\n",
        "\n",
        "def occurence(s, first, second):\n",
        "\n",
        "  \"\"\"\n",
        "  In this function 2 arguments are passed in the function.\n",
        "  If these two characters are consecutive in s then the count is incremented.\n",
        "  And it will provide us with how many times they are occuring consecutively.\n",
        "  \"\"\"\n",
        "  c = 0\n",
        "  for i in range(len(s)-1):\n",
        "    if s[i] == first and s[i+1] == second:\n",
        "      c += 1\n",
        "  return c\n",
        "\n",
        "def occurence1(s, first, second, third):\n",
        "  \"\"\"\n",
        "  In this function 3 arguments are passed in the function.\n",
        "  If these three characters are consecutive in s then the count is incremented.\n",
        "  And it will provide us with how many times they are occuring consecutively.\n",
        "  \"\"\"\n",
        "  c = 0\n",
        "  for i in range(len(s)-1):\n",
        "    if s[i] == first and s[i+1] == second and s[i+2] == third:\n",
        "      c += 1\n",
        "  return c\n",
        "\n",
        "\n",
        "def markov_estimate(s):\n",
        "  \"\"\"\n",
        "  This function calculates min_entropy of the input s by markov_estimate.\n",
        "  @input: s: input data\n",
        "  @output:min_entropy : min_entropy of the input s\n",
        "  \"\"\"\n",
        "\n",
        "  #STEP 1: p0 and p1 are  calculated using given equation.\n",
        "  cnt0 = count(s, 0) # count function is used here for calculating the number of 0 in s.\n",
        "  p0 = cnt0/len(s) # p0 is probability of 0 in s.\n",
        "  p1 = 1 - p0 # p1 is probability of 1 in s.\n",
        "  P = {}\n",
        "\n",
        "  #STEP 2: p00, p11, p10, p01 are calculated here using count and occurence function.\n",
        "  cnt00 = occurence(s, 0, 0) #occurence function is used to calculate the count of 00 in s.\n",
        "  cnt01 = occurence(s, 0, 1)\n",
        "  cnt10 = occurence(s, 1, 0)\n",
        "  cnt11 = occurence(s, 1, 1)\n",
        "  p00  = (cnt00  / (cnt00 + cnt01)) #p00 is probability of 00 in s, calculated using count of 00 and 01.\n",
        "  p01  = (cnt01 / (cnt01 + cnt00 ))\n",
        "  p10  = (cnt10 / (cnt10 + cnt11))\n",
        "  p11  = (cnt11 / (cnt11 + cnt10 ))\n",
        "\n",
        "  p_seq = [0.0 for x in range(6)] #STEP 3: The probability of the most likely sequence of outputs of length 128, as calculated below:\n",
        "  p_seq[0] = p0 * (p00**127)\n",
        "  p_seq[1] = p0 * pow(p01,64) * pow(p10,63)\n",
        "  p_seq[2] = p0 * p01 * (pow(p11,126))\n",
        "  p_seq[3] = p1 * p10 * (pow(p00,126))\n",
        "  p_seq[4] = p1 * pow(p10,64) * pow(p01,63)\n",
        "  p_seq[5] = p1 * pow(p11,127)\n",
        "\n",
        "\n",
        "  p_max = max(p_seq) #STEP 4 : Let p_max be the maximum of the probabilities in the table given above\n",
        "  min_entropy = -math.log(p_max,2)/128.0 #Step 5: The min-entropy estimate is the negative logarithm of the probability of the most likely sequence of outputs p_max\n",
        "  if min_entropy > 1.0:\n",
        "    min_entropy = 1.0\n",
        "\n",
        "  return  min_entropy\n",
        "\n",
        "s = (1,0,0,0,1,1,1,0,0,1,0,1,0,1,0,1,1,1,0,0,1,1,0,0,0,1,1,1,0,0,1,0,1,0,1,0,1,1,1,0)\n",
        "min_entropy = markov_estimate(s)\n",
        "print(min_entropy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS2SVAUWAoBc",
        "outputId": "6b6ff3f4-78b4-4f6d-c385-f52f5cca95bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7606360062539939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) The Compression Estimate"
      ],
      "metadata": {
        "id": "DIfumJi2D9Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\"This function converts binary number to integer\"\n",
        "def bits_to_int(bits):\n",
        "  theint = 0\n",
        "  for i in range(len(bits)):\n",
        "    theint = (theint << 1) + bits[i]\n",
        "  return theint\n",
        "\n",
        "def F(z,t,u):\n",
        "  if u < t:\n",
        "    return (z**2.0)*((1.0-z)**(u-1.0))\n",
        "  if u == t:\n",
        "    return z*((1.0-z)**(t-1.0))\n",
        "\n",
        "def G(z, v, d, L):\n",
        "  G_sum = 0.0\n",
        "  st = [math.log(u, 2.0) * ((1.0-z)**(u-1.0)) for u in range((d+1), v+d+1)]\n",
        "  G_sum = v*z*z * sum([math.log(u, 2.0) * ((1.0-z)**(u-1.0)) for u in range(1,(d+1))])\n",
        "  G_sum += z*z * sum([(v-t-1) * st[t] for t in range(v-1)])\n",
        "  G_sum += z * sum(st)\n",
        "  return G_sum/v\n",
        "\n",
        "def compression_estimate(bits, d=4):\n",
        "  \"\"\"\n",
        "  This function calculates min_entropy of the input bits by compression_estimate.\n",
        "  Given a dataset from the noise source, the samples are first partitioned into two disjoint groups.\n",
        "  The first group serves as the dictionary for the compression algorithm; the second group is used\n",
        "  as the test group. The compression values are calculated over the test group to determine the mean,\n",
        "  which is the Maurer statistic. Using the same method as the collision estimate, the probability\n",
        "  distribution that has the minimum possible entropy for the calculated Maurer statistic is\n",
        "  determined. For this distribution, the entropy per sample is calculated as the lower bound on the\n",
        "  entropy that is present.\n",
        "  @input:bits : input elements\n",
        "  @output:min_entropy : min_entropy of the input bits\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(bits)\n",
        "  print(\"COMPRESSION Test\")\n",
        "  #STEP 1: create a new list s_dash = [s_dash1, s_dash2,.....,s_dash[L/b]] by dividing L into non-overlapping b-bit blocks. If L is not a multiple of b, discard the extra data.\n",
        "  b = 6\n",
        "  blocks = L//b\n",
        "  s_dash = [0,]+[bits_to_int(bits[b*i:b*(i+1)]) for i in range(blocks)]\n",
        "\n",
        "  #STEP 2: Partition the dataset s_dash, into two disjoint groups. These two groups will form the dictionary_data and the testing_data.\n",
        "  #STEP 2a: Create the dictionary from the first d = 4 elements of s_dash, [s_dash1,s_dash2,.....,s_dashd]\n",
        "  dictionary_data = s_dash[1:d+1]\n",
        "  v = blocks-d\n",
        "  #STEP 2b: Use the remaining v = ⌊L/b⌋ − d observations i,e, 4, (s_dash𝑑+1′ , … , s_dash⌊L/b⌋), for testing\n",
        "  testing_data=s_dash[d+1:]\n",
        "\n",
        "\n",
        "  print(\"   v                   \",v)\n",
        "\n",
        "  #STEP 3: Initialize the dictionary to an all zero array of size 2**b\n",
        "  dictionary = [0 for i in range((2**b)+1)] # Make it 1 bigger and leave the zero element dangling\n",
        "                                              # so the indexes match the spec which uses 1 based arrays.\n",
        "  for i in range(1,d+1):\n",
        "      dictionary[s_dash[i]]=i\n",
        "\n",
        "  #STEP 4: Run the test data against the dictionary_data created in Step 2.\n",
        "  #STEP 4a: Let D be a list of length v.\n",
        "  #STEP 4b: For i from d + 1 to ⌊L/b⌋\n",
        "  D = [0,]+[0 for i in range(v)]\n",
        "  for i in range(d+1,blocks+1):\n",
        "    if dictionary[s_dash[i]] != 0: #steps 4b(1)\n",
        "      D[i-d] = i-dictionary[s_dash[i]]\n",
        "      dictionary[s_dash[i]] = i\n",
        "    if dictionary[s_dash[i]] == 0: #steps 4b(2)\n",
        "      dictionary[s_dash[i]] = i\n",
        "      D[i-d] = i\n",
        "\n",
        "  # STEP 5: Calculating sample mean, x_bar\n",
        "\n",
        "  add = 0.0\n",
        "  for i in range(1,v+1):\n",
        "    add += math.log(D[i],2)\n",
        "    X_bar = add/v\n",
        "\n",
        "  print(\"   X_bar               \",X_bar)\n",
        "\n",
        "  c = 0.5907\n",
        "\n",
        "  d_add = 0.0 # declaration of a variable for calculating standard deviation\n",
        "  for i in range(1,v+1):\n",
        "    d_add += (math.log(D[i],2)**2)\n",
        "  d_add = d_add/(v-1.0)\n",
        "  d_add = d_add - (X_bar**2)\n",
        "  stand_devi = c * math.sqrt(d_add) # calculating standard deviation\n",
        "\n",
        "  print(\"   stand_devi          \",stand_devi)\n",
        "\n",
        "  # STEP 6: Compute the lower-bound of the confidence interval for the mean, based on a normal distribution\n",
        "\n",
        "  X_bar_dash = X_bar - ((2.576*stand_devi)/math.sqrt(v))\n",
        "  print(\"   X_bar_dash         \",X_bar_dash)\n",
        "\n",
        "  # STEP 7: Using a binary search, solve for the parameter P\n",
        "\n",
        "  p_min = 2.0 ** -b  # binary search bounds\n",
        "  p_max = 1.0\n",
        "  p_mid = (p_min+p_max)/2.0\n",
        "  iterations = 1000\n",
        "  iteration = 0\n",
        "\n",
        "  found = False\n",
        "  while (iteration < iterations):\n",
        "    q = (1.0-p_mid)/((2.0**b)-1.0)\n",
        "    candidate = G(p_mid,v,d,L) + (((2.0**b)-1.0)*G(q,v,d,L))\n",
        "\n",
        "    if abs(candidate -X_bar_dash) < 0.00000000001:\n",
        "      found = True\n",
        "      break\n",
        "    elif candidate > X_bar_dash:\n",
        "      p_min = p_mid\n",
        "      p_mid = (p_min+p_max)/2.0\n",
        "    elif candidate < X_bar_dash:\n",
        "      p_max = p_mid\n",
        "      p_mid = (p_min+p_max)/2.0\n",
        "\n",
        "      iteration += 1\n",
        "\n",
        "  print(\"   p          =\",p_mid)\n",
        "  # STEP 8: If the binary search yields a solution, then the min-entropy is the negative logarithm of the parameter p , otherwise its 1.\n",
        "  if found:\n",
        "    min_entropy = -math.log(p_mid,2)/b\n",
        "    print(\"   min_entropy =\",min_entropy)\n",
        "    return min_entropy\n",
        "  else:\n",
        "    min_entropy = 1.0\n",
        "    print(\"   min_entropy = 1.0\")\n",
        "    return min_entropy\n",
        "\n",
        "bits = (1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1 ,1, 1, 0, 0, 0, 1, 1)\n",
        "min_entropy = compression_estimate(bits)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLngAkYe9W_O",
        "outputId": "e2027e80-2b58-4f30-c44b-43d35f7d5471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPRESSION Test\n",
            "   v                    4\n",
            "   X_bar                2.6304001099309313\n",
            "   stand_devi           0.9073768771997716\n",
            "   X_bar_dash          1.4616986920976256\n",
            "   p          = 0.5714846826669486\n",
            "   min_entropy = 0.13453554400886117\n"
          ]
        }
      ]
    }
  ]
}